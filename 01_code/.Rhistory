`01_main_data` <- readRDS("C:/Users/sanyl/OneDrive - Universidad de los Andes/Documentos/2.PEG-UNIANDES/7.Bigdata_MachineL/PSET_1_BDML_2025_02_G2/02_prepare_data/03_output/01_main_data.rds")
#======================#
# Sany Leon
# CDATE:  02/01/2026
# MDATE:  02/01/2026
#======================#
# setup
rm(list = ls())
pacman::p_load(rio, rvest, tidyverse, data.table)
url <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/"
#--- Data
page <- read_html(url)
# obtener links internos de pagina principal
links <- page %>%
html_elements("a") %>%
html_attr("href") %>%
c() %>%
.[str_detect(string = ., pattern = "page")] %>%
paste0(url,.)
# ingresar a pagina y hacer scrapping
data = map(.x = links, .f = function(x){
Sys.sleep(5)
link = read_html(x) %>%
as.character(page) %>%
str_extract_all(pattern = "pages/geih_page.+\\.html") %>%
unlist() %>%
paste0(url, .)
# importar datos
data = read_html(link) %>%
html_table()
return(data[[1]])
})
data = rbindlist(data)
#--- dictionaries
dict = read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/dictionary.html") %>%
html_table() %>%
.[[1]]
labels = read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/labels.html") %>%
html_table() %>%
.[[1]]
dictionary = list("dictionarie" = dict,
"labels" = labels)
export(data, "data_output/01_data_scrapping_web_page.rds")
export(dictionary, "dictionary.xlsx")
export(dictionary, "dictionary.xlsx")
View(dictionary)
View(dictionary)
View(labels)
View(page)
View(data)
`01_data_scrapping_web_page` <- readRDS("C:/Users/sanyl/OneDrive - Universidad de los Andes/Documentos/2.PEG-UNIANDES/7.Bigdata_MachineL/PS1_2026/BDML-PS08/01_code/data_output/01_data_scrapping_web_page.rds")
export(dictionary, "99_additional/dictionary.xlsx")
export(dictionary, "99_additional/dictionary.xlsx")
export(dictionary, "99_additional/dictionary.xlsx")
#========================================================#
# Sany, Andres and Juan
# Fisrt Created:  02/01/2026
# Last update:  02/01/2026
# Script:        Scrapping data from Ignacio repository
#======================================================#
# setup
rm(list = ls())
rm(list = ls())
pacman::p_load(rio, rvet, tidyverse, data.table)
#=====================#
# Import
#=====================#
url <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/"
#=====================#
# scrapping
#=====================#
#--- Data
page <- read_html(url)
#========================================================#
# Sany, Andres and Juan
# Fisrt Created:  02/01/2026
# Last update:  02/01/2026
# Script:        Scrapping data from Ignacio repository
#======================================================#
# setup
rm(list = ls())
rm(list = ls())
pacman::p_load(rio, rvet, tidyverse, data.table)
#=====================#
# Import
#=====================#
url <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/"
#=====================#
# scrapping
#=====================#
#--- Data
page <- read_html(url)
page <- read_html(url)
#========================================================#
# Sany, Andres and Juan
# Fisrt Created:  02/01/2026
# Last update:  02/01/2026
# Script:        Scrapping data from Ignacio repository
#======================================================#
# Setup
rm(list = ls())
rm(list = ls())
pacman::p_load(rio, rvet, tidyverse, data.table)
#=====================#
# Import
#=====================#
url <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/"
page <- read_html(url)
#======================#
# Alejandro Cardiles
# CDATE:  2025-08-28
# MDATE:  2025-08-29
#======================#
# setup
rm(list = ls())
pacman::p_load(rio, rvest, tidyverse, data.table)
#=====================#
# Import
#=====================#
url <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/"
#=====================#
# scrapping
#=====================#
#--- Data
page <- read_html(url)
#========================================================#
# Sany, Andres and Juan
# Fisrt Created:  02/01/2026
# Last update:  02/01/2026
# Script:       Scrapping data from Ignacio repository
#======================================================#
# Setup
rm(list = ls())
rm(list = ls())
pacman::p_load(rio, rvet, tidyverse, data.table)
#=====================#
# Import
#=====================#
url <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/"
#=====================#
# scrapping
#=====================#
#--- Data
page <- read_html(url)
#========================================================#
# Sany, Andres and Juan
# Fisrt Created:  02/01/2026
# Last update:  02/01/2026
# Script:       Scrapping data from Ignacio repository
#======================================================#
# Setup
rm(list = ls())
rm(list = ls())
pacman::p_load(rio, rvet, tidyverse, data.table)
#=====================#
# Import
#=====================#
url <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/"
#=====================#
# scrapping
#=====================#
#--- Data
page <- read_html(url)
#get in page and scrapping
data = map(.x = links, .f = function(x){
Sys.sleep(5)
link = read_html(x) %>%
as.character(page) %>%
str_extract_all(pattern = "pages/geih_page.+\\.html") %>%
unlist() %>%
paste0(url, .)
# import data
data = read_html(link) %>%
html_table()
return(data[[1]])
})
data = rbindlist(data)
#--- dictionaries
dict = read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/dictionary.html") %>%
html_table() %>%
.[[1]]
#========================================================#
# Sany, Andres and Juan
# Fisrt Created:  02/01/2026
# Last update:  02/01/2026
# Script:       Scrapping data from Ignacio repository
#======================================================#
# Setup
rm(list = ls())
rm(list = ls())
pacman::p_load(rio, rvet, tidyverse, data.table)
#=====================#
# Import
#=====================#
url <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/"
#=====================#
# scrapping
#=====================#
#--- Data
page <- read_html(url)
# Get links from main page
links <- page %>%
html_elements("a") %>%
html_attr("href") %>%
c() %>%
.[str_detect(string = ., pattern = "page")] %>%
paste0(url,.)
#get in page and scrapping
data = map(.x = links, .f = function(x){
Sys.sleep(5)
link = read_html(x) %>%
as.character(page) %>%
str_extract_all(pattern = "pages/geih_page.+\\.html") %>%
unlist() %>%
paste0(url, .)
# import data
data = read_html(link) %>%
html_table()
return(data[[1]])
})
data = rbindlist(data)
#--- dictionaries
dict = read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/dictionary.html") %>%
html_table() %>%
.[[1]]
labels = read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/labels.html") %>%
html_table() %>%
.[[1]]
dictionary = list("dictionarie" = dict,
"labels" = labels)
#=====================#
# export
#=====================#
export(data, "00_data/01_data_scrapping_web_page.rds")
export(dictionary, "99_additional/dictionary.xlsx")
#========================================================#
# Sany, Andres and Juan
# Fisrt Created:  02/01/2026
# Last update:  02/01/2026
# Script:       Scrapping data from Ignacio repository
#======================================================#
# Setup
rm(list = ls())
rm(list = ls())
pacman::p_load(rio, rvet, tidyverse, data.table)
#=====================#
# Import
#=====================#
url <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/"
#=====================#
# scrapping
#=====================#
#--- Data
page <- read_html(url)
# Get links from main page
links <- page %>%
html_elements("a") %>%
html_attr("href") %>%
c() %>%
.[str_detect(string = ., pattern = "page")] %>%
paste0(url,.)
#get in page and scrapping
data = map(.x = links, .f = function(x){
Sys.sleep(5)
link = read_html(x) %>%
as.character(page) %>%
str_extract_all(pattern = "pages/geih_page.+\\.html") %>%
unlist() %>%
paste0(url, .)
# import data
data = read_html(link) %>%
html_table()
return(data[[1]])
})
data = rbindlist(data)
#--- dictionaries
dict = read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/dictionary.html") %>%
html_table() %>%
.[[1]]
labels = read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/labels.html") %>%
html_table() %>%
.[[1]]
dictionary = list("dictionarie" = dict,
"labels" = labels)
#=====================#
# export
#=====================#
export(data, "00_data/01_data_scrapping_web_page.rds")
export(dictionary, "99_additional/dictionary.xlsx")
#========================================================#
# PS Prediction Income
# Sany, Andres and Juan
# Fisrt Created:  02/01/2026
# Last update:  02/01/2026
# Script:       Scrapping data from Ignacio repository
#======================================================#
# Setup
rm(list = ls())
rm(list = ls())
pacman::p_load(rio, rvet, tidyverse, data.table)
#=====================#
# Import
#=====================#
url <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/"
#=====================#
# scrapping
#=====================#
#--- Data
page <- read_html(url)
# Get links from main page
links <- page %>%
html_elements("a") %>%
html_attr("href") %>%
c() %>%
.[str_detect(string = ., pattern = "page")] %>%
paste0(url,.)
#get in page and scrapping
data = map(.x = links, .f = function(x){
Sys.sleep(5)
link = read_html(x) %>%
as.character(page) %>%
str_extract_all(pattern = "pages/geih_page.+\\.html") %>%
unlist() %>%
paste0(url, .)
# import data
data = read_html(link) %>%
html_table()
return(data[[1]])
})
data = rbindlist(data)
#--- dictionaries
dict = read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/dictionary.html") %>%
html_table() %>%
.[[1]]
labels = read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/labels.html") %>%
html_table() %>%
.[[1]]
dictionary = list("dictionarie" = dict,
"labels" = labels)
#=====================#
# export
#=====================#
export(data, "BDML-PS08/00_data/01_data_scrapping_web_page.rds")
export(dictionary, "BDML-PS08/99_additional/dictionary.xlsx")
source("01_code/01_data_scraper.R")
source("01_code/01_data_scraper.R")
source("01_code/01_data_scraper.R")
# Step 1: Download and construct the raw dataset
library(here)
install.packages(here)
install.packages(here)
install.packages(here)
install.packages("here")
library(here)
source(here("01_code", "01_data_scraper.R"))
install.packages("here")
# Step 1: Download and construct the raw dataset
library(here)
source("01_code/01_data_scraper.R")
# Step 1: Download and construct the raw dataset
library(here)
source("01_code/01_data_scraper.R")
install.packages("here")
install.packages("here")
